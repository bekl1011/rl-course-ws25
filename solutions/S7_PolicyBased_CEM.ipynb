{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Lunar Lander with Cross-Entropy Method\n",
    "\n",
    "In this notebook we look at the lunar lander environment and solve it with the cross-entropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "59a0cc23-613d-4378-8de6-2b4d280e9fa9"
   },
   "outputs": [],
   "source": [
    "#!pip3 install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhZ0fzBkKmAA"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "We define a simple neural network that generates the action scores based on a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vWQr7TZgKmAB"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zFMlVViKmAE"
   },
   "source": [
    "# Generate Episodes\n",
    "\n",
    "We generate a batch of episodes and remember the traversed states, actions and rewards. To select the next action we use the output of the network. For this we first pass the scores through a softmax to get probabilites. In the second step we sampel from this distribution to get the next action to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AIiayltZKmAF"
   },
   "outputs": [],
   "source": [
    "def generate_batch(env, batch_size, t_max=5000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states, actions = [], []\n",
    "        total_reward = 0\n",
    "        s, _ = env.reset()\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "            new_s, r, done, _, _ = env.step(a)\n",
    "\n",
    "            # record sessions like you did before\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvq2ZIvlKmAJ"
   },
   "source": [
    "# Training\n",
    "\n",
    "In the training step, we first use the neural network to generate a batch of episodes and then use the state-action pairs to improve the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFUzEnaDKmAJ",
    "outputId": "a5344f76-e542-4566-808e-8864fcdd4f09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/8hnx45y520d4nlwgykzcbypc0000gn/T/ipykernel_20571/2609643390.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_69nk78ncaj/croot/pytorch_1669252638507/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  s_v = torch.FloatTensor([s])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.417, reward_mean=-177.1\n",
      "1: loss=1.394, reward_mean=-160.9\n",
      "2: loss=1.359, reward_mean=-157.7\n",
      "3: loss=1.374, reward_mean=-112.8\n",
      "4: loss=1.366, reward_mean=-113.7\n",
      "5: loss=1.326, reward_mean=-120.6\n",
      "6: loss=1.365, reward_mean=-101.1\n",
      "7: loss=1.266, reward_mean=-100.8\n",
      "8: loss=1.372, reward_mean=-100.5\n",
      "9: loss=1.357, reward_mean=-94.2\n",
      "10: loss=1.310, reward_mean=-83.6\n",
      "11: loss=1.204, reward_mean=-86.4\n",
      "12: loss=1.290, reward_mean=-74.0\n",
      "13: loss=1.247, reward_mean=-52.2\n",
      "14: loss=1.247, reward_mean=-39.2\n",
      "15: loss=1.180, reward_mean=-26.0\n",
      "16: loss=1.214, reward_mean=-26.4\n",
      "17: loss=1.230, reward_mean=-29.9\n",
      "18: loss=1.042, reward_mean=-22.7\n",
      "19: loss=1.219, reward_mean=-23.5\n",
      "20: loss=1.075, reward_mean=1.2\n",
      "21: loss=1.072, reward_mean=-10.9\n",
      "22: loss=1.088, reward_mean=-9.9\n",
      "23: loss=1.065, reward_mean=-27.3\n",
      "24: loss=1.017, reward_mean=-1.8\n",
      "25: loss=1.106, reward_mean=0.7\n",
      "26: loss=0.976, reward_mean=4.6\n",
      "27: loss=1.083, reward_mean=11.1\n",
      "28: loss=1.072, reward_mean=13.6\n",
      "29: loss=0.999, reward_mean=19.7\n",
      "30: loss=0.993, reward_mean=20.3\n",
      "31: loss=1.037, reward_mean=15.2\n",
      "32: loss=1.144, reward_mean=12.6\n",
      "33: loss=0.951, reward_mean=24.6\n",
      "34: loss=1.009, reward_mean=20.1\n",
      "35: loss=1.156, reward_mean=23.1\n",
      "36: loss=1.088, reward_mean=31.4\n",
      "37: loss=1.033, reward_mean=31.9\n",
      "38: loss=0.876, reward_mean=39.4\n",
      "39: loss=1.114, reward_mean=34.0\n",
      "40: loss=1.042, reward_mean=35.7\n",
      "41: loss=1.080, reward_mean=34.4\n",
      "42: loss=0.946, reward_mean=39.3\n",
      "43: loss=0.956, reward_mean=31.5\n",
      "44: loss=0.943, reward_mean=30.5\n",
      "45: loss=1.083, reward_mean=29.6\n",
      "46: loss=0.981, reward_mean=32.1\n",
      "47: loss=1.108, reward_mean=33.8\n",
      "48: loss=0.950, reward_mean=32.9\n",
      "49: loss=1.013, reward_mean=30.0\n",
      "50: loss=0.971, reward_mean=37.5\n",
      "51: loss=0.937, reward_mean=36.0\n",
      "52: loss=1.036, reward_mean=38.1\n",
      "53: loss=0.904, reward_mean=29.3\n",
      "54: loss=0.931, reward_mean=24.7\n",
      "55: loss=0.869, reward_mean=34.7\n",
      "56: loss=1.006, reward_mean=30.4\n",
      "57: loss=1.017, reward_mean=36.5\n",
      "58: loss=0.822, reward_mean=38.2\n",
      "59: loss=1.096, reward_mean=35.4\n",
      "60: loss=0.653, reward_mean=36.3\n",
      "61: loss=0.855, reward_mean=44.0\n",
      "62: loss=0.944, reward_mean=40.4\n",
      "63: loss=0.823, reward_mean=42.8\n",
      "64: loss=0.969, reward_mean=41.9\n",
      "65: loss=0.816, reward_mean=48.2\n",
      "66: loss=0.792, reward_mean=40.8\n",
      "67: loss=1.001, reward_mean=40.6\n",
      "68: loss=0.953, reward_mean=38.4\n",
      "69: loss=0.937, reward_mean=40.2\n",
      "70: loss=0.869, reward_mean=41.1\n",
      "71: loss=0.935, reward_mean=37.9\n",
      "72: loss=0.941, reward_mean=44.8\n",
      "73: loss=0.755, reward_mean=41.0\n",
      "74: loss=0.911, reward_mean=43.4\n",
      "75: loss=0.885, reward_mean=38.7\n",
      "76: loss=0.941, reward_mean=29.7\n",
      "77: loss=0.629, reward_mean=26.5\n",
      "78: loss=0.878, reward_mean=18.8\n",
      "79: loss=0.931, reward_mean=25.4\n",
      "80: loss=0.593, reward_mean=28.4\n",
      "81: loss=1.051, reward_mean=30.5\n",
      "82: loss=0.763, reward_mean=31.8\n",
      "83: loss=0.836, reward_mean=34.7\n",
      "84: loss=0.952, reward_mean=34.3\n",
      "85: loss=0.871, reward_mean=32.7\n",
      "86: loss=0.822, reward_mean=39.9\n",
      "87: loss=1.076, reward_mean=27.7\n",
      "88: loss=0.966, reward_mean=36.6\n",
      "89: loss=0.813, reward_mean=30.7\n",
      "90: loss=0.936, reward_mean=34.5\n",
      "91: loss=0.668, reward_mean=34.1\n",
      "92: loss=0.811, reward_mean=32.4\n",
      "93: loss=1.006, reward_mean=37.9\n",
      "94: loss=0.832, reward_mean=58.7\n",
      "95: loss=0.803, reward_mean=36.2\n",
      "96: loss=0.841, reward_mean=28.9\n",
      "97: loss=0.881, reward_mean=51.3\n",
      "98: loss=0.952, reward_mean=37.4\n",
      "99: loss=0.778, reward_mean=53.6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 100\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "completion_score = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    \n",
    "    # generate new episodes\n",
    "    states, actions, rewards = generate_batch(env, batch_size, t_max=500)\n",
    "    \n",
    "    threshold = np.percentile(rewards, percentile)\n",
    "    \n",
    "    # train on the states using actions as targets\n",
    "    for s_i in range(len(states)):\n",
    "        if rewards[s_i] < threshold: # skip this iteration if rewards are too low\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        tensor_states = torch.FloatTensor(states[s_i])\n",
    "        tensor_actions = torch.LongTensor(actions[s_i])\n",
    "        action_scores_v = net(tensor_states)\n",
    "        loss_v = objective(action_scores_v, tensor_actions)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klausbertsch/miniconda3/envs/rl-course/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/klausbertsch/Documents/GitHub/rl-course-ws25/solutions/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/klausbertsch/Documents/GitHub/rl-course-ws25/solutions/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/klausbertsch/Documents/GitHub/rl-course-ws25/solutions/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/klausbertsch/Documents/GitHub/rl-course-ws25/solutions/video/rl-video-episode-0.mp4\n",
      "Total reward: 177.56683536236716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"video\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "        \n",
    "        state = torch.FloatTensor([state])\n",
    "        action_scores_v = net(state)\n",
    "        action = np.argmax(action_scores_v.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "6_LunarLander_PolicyBased.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
